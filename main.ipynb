{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: youtube-transcript-api in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: chromadb in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.16)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from youtube-transcript-api) (0.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from youtube-transcript-api) (2.32.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.173.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.10.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->youtube-transcript-api) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->youtube-transcript-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->youtube-transcript-api) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->youtube-transcript-api) (2025.1.31)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (2.1.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hazem\\appdata\\roaming\\python\\python313\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: sympy in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic->google-generativeai) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hazem\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.34.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.2.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hazem\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install youtube-transcript-api google-generativeai chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"my_vectordb\")\n",
    "\n",
    "ollama_ef = embedding_functions.OllamaEmbeddingFunction(\n",
    "    model_name=\"nomic-embed-text\",  \n",
    ")\n",
    "\n",
    "chroma_collection = chroma_client.get_or_create_collection(\n",
    "    name='yt_notes',\n",
    "    embedding_function=ollama_ef\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_video_id = 'bCz4OMemCcA'\n",
    "\n",
    "prompt = \"Extract key notes from video transcript: \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytt_api = YouTubeTranscriptApi()\n",
    "\n",
    "# Fetch transcript (English priority)\n",
    "fetched_transcript = ytt_api.fetch(yt_video_id, languages=['en', 'en-US', 'en-GB'])\n",
    "\n",
    "# Convert to plain text\n",
    "transcript_text = \"\\n\".join([snippet.text for snippet in fetched_transcript])\n",
    "\n",
    "# Save to file\n",
    "with open(\"temp_transcript.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(transcript_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an API instance\n",
    "ytt_api = YouTubeTranscriptApi()\n",
    "\n",
    "# Fetch transcript (English priority)\n",
    "fetched_transcript = ytt_api.fetch(yt_video_id, languages=['en', 'en-US', 'en-GB'])\n",
    "\n",
    "# Convert transcript objects to plain text\n",
    "transcript_text = \"\\n\".join([snippet.text for snippet in fetched_transcript])\n",
    "\n",
    "# Generate notes using Ollama\n",
    "response = ollama.chat(model=\"llama3\", messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt + transcript_text}\n",
    "])\n",
    "notes = response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['bCz4OMemCcA'], 'embeddings': None, 'documents': ['Here are the key notes from the video transcript:\\n\\n**Introduction**\\n\\n* The Transformer model is an attention-based neural network architecture designed for machine translation.\\n* This video will go through each aspect of the Transformer model.\\n\\n**Encoder-Decoder Structure**\\n\\n* The Transformer consists of an encoder and a decoder.\\n* The encoder takes in a sequence of input tokens and outputs a sequence of hidden states.\\n* The decoder takes in the output from the encoder and generates a sequence of output tokens.\\n\\n**Self-Attention Mechanism**\\n\\n* Self-attention is used to allow the model to attend to different parts of the input sequence simultaneously.\\n* This mechanism helps the model capture long-range dependencies and contextual relationships between tokens.\\n\\n**Multi-Head Attention**\\n\\n* Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\\n* This helps the model capture more complex relationships between tokens.\\n\\n**Causal Masking**\\n\\n* Causal masking is used to prevent the decoder from attending to future tokens before they are generated.\\n* This ensures that the model generates output tokens in a causal manner, one step at a time.\\n\\n**Inference**\\n\\n* During inference, the model takes in an input sequence and outputs a translated sequence of tokens.\\n* The model uses the encoder-decoder structure and self-attention mechanism to generate each token in the output sequence.\\n* In this example, the model is translating English sentences into Italian.\\n\\n**Training**\\n\\n* Training involves feeding the model pairs of input-output sequences and optimizing the loss function using backpropagation.\\n* The cross-entropy loss is used as the optimization objective.\\n\\n**Inference Strategy**\\n\\n* Two inference strategies are discussed: greedy search and beam search.\\n* Greedy search takes the maximum soft value at each step, while beam search explores multiple possible paths and selects the most likely one.\\n\\n**Conclusion**\\n\\n* The Transformer model has been successful in various natural language processing tasks, including machine translation.\\n* This video has covered the basics of the Transformer model, from its architecture to inference.'], 'uris': None, 'included': ['documents'], 'data': None, 'metadatas': None}\n"
     ]
    }
   ],
   "source": [
    "# Save transcript \n",
    "with open(\"temp_transcript.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(transcript_text)\n",
    "\n",
    "# Save notes\n",
    "with open(\"temp_notes.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(notes)\n",
    "\n",
    "# Read notes back in\n",
    "with open(\"temp_notes.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    notes = file.read()\n",
    "\n",
    "# Insert or update record in Chroma\n",
    "chroma_collection.upsert(\n",
    "    documents=[notes],\n",
    "    ids=[yt_video_id]\n",
    ")\n",
    "\n",
    "# Validation\n",
    "result = chroma_collection.get(yt_video_id, include=['documents'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top Search Results:\n",
      "\n",
      "************************************************************************\n",
      "1.  https://youtu.be/bCz4OMemCcA\n",
      "************************************************************************\n",
      "Here are the key notes from the video transcript:\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "* The Transformer model is an attention-based neural network architecture designed for machine translation.\n",
      "* This video will go through each aspect of the Transformer model.\n",
      "\n",
      "**Encoder-Decoder Structure**\n",
      "\n",
      "* The Transformer consists of an encoder and a decoder.\n",
      "* The encoder takes in a sequence of input tokens and outputs a sequence of hidden states.\n",
      "* The decoder takes in the output from the encoder and generates a sequence of output tokens.\n",
      "\n",
      "**Self-Attention Mechanism**\n",
      "\n",
      "* Self-attention is used to allow the model to attend to different parts of the input sequence simultaneously.\n",
      "* This mechanism helps the model capture long-range dependencies and contextual relationships between tokens.\n",
      "\n",
      "**Multi-Head Attention**\n",
      "\n",
      "* Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "* This helps the model capture more complex relationships between tokens.\n",
      "\n",
      "**Causal Masking**\n",
      "\n",
      "* Causal masking is used to prevent the decoder from attending to future tokens before they are generated.\n",
      "* This ensures that the model generates output tokens in a causal manner, one step at a time.\n",
      "\n",
      "**Inference**\n",
      "\n",
      "* During inference, the model takes in an input sequence and outputs a translated sequence of tokens.\n",
      "* The model uses the encoder-decoder structure and self-attention mechanism to generate each token in the output sequence.\n",
      "* In this example, the model is translating English sentences into Italian.\n",
      "\n",
      "**Training**\n",
      "\n",
      "* Training involves feeding the model pairs of input-output sequences and optimizing the loss function using backpropagation.\n",
      "* The cross-entropy loss is used as the optimization objective.\n",
      "\n",
      "**Inference Strategy**\n",
      "\n",
      "* Two inference strategies are discussed: greedy search and beam search.\n",
      "* Greedy search takes the maximum soft value at each step, while beam search explores multiple possible paths and selects the most likely one.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "* The Transformer model has been successful in various natural language processing tasks, including machine translation.\n",
      "* This video has covered the basics of the Transformer model, from its architecture to inference. \n",
      "\n",
      "************************************************************************\n",
      "2.  https://youtu.be/hQH4-5o0BMM\n",
      "************************************************************************\n",
      "Here are the key notes extracted from the video transcript:\n",
      "\n",
      "**Spaghetti and Meat Sauce Recipe**\n",
      "\n",
      "* Only takes 30 minutes to prepare\n",
      "* Healthy and loaded with veggies\n",
      "* Delicious and full of flavor\n",
      "* Can be made for dinner any night of the week\n",
      "\n",
      "**Prepping Veggies**\n",
      "\n",
      "* Chop onion, carrot, celery into similar size pieces using a food processor\n",
      "* Add garlic (5 cloves) and process until well combined\n",
      "* Use leaves from celery stalks for added sweetness\n",
      "\n",
      "**Cooking Meat Sauce**\n",
      "\n",
      "* Cook chopped veggies in a large pot with olive oil, salt, and Italian seasoning\n",
      "* Add ground beef and Italian sausage, cook until meat is browned and vegetables are caramelized\n",
      "* Add tomato paste, crushed tomatoes, fish sauce (optional), and stir to combine\n",
      "* Simmer for 15 minutes\n",
      "\n",
      "**Finishing the Sauce**\n",
      "\n",
      "* Scrape off any caramelization at the bottom of the pot and add pasta water to help sauce cling to spaghetti\n",
      "* Finish cooking pasta in the meat sauce\n",
      "\n",
      "**Assembling and Serving**\n",
      "\n",
      "* Add grated Parmesan cheese, chopped parsley and basil\n",
      "* Serve with a spoonful of extra meat sauce on top (if desired)\n",
      "* Add red pepper flakes for an extra kick of spiciness\n",
      "\n",
      "Overall, this recipe is quick, easy, and packed with flavor! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define query parameters\n",
    "query_text = \"How does the transformer work and what is the difference between the transformer and the RNN\"\n",
    "n_results = 5\n",
    "\n",
    "# Perform semantic search\n",
    "results = chroma_collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=n_results,\n",
    "    include=[\"documents\", \"distances\", \"metadatas\"]\n",
    ")\n",
    "\n",
    "# Display search results\n",
    "print(\"\\n🔍 Top Search Results:\\n\")\n",
    "for idx, (vid_id, doc) in enumerate(zip(results[\"ids\"][0], results[\"documents\"][0]), start=1):\n",
    "    print(f\"{'*' * 72}\")\n",
    "    print(f\"{idx}.  https://youtu.be/{vid_id}\")\n",
    "    print(f\"{'*' * 72}\")\n",
    "    print(doc, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the document, here's how the Transformer works:\n",
      "\n",
      "The Transformer is an attention-based neural network architecture designed for machine translation. It consists of an encoder and a decoder. The encoder takes in a sequence of input tokens and outputs a sequence of hidden states. The decoder then takes in the output from the encoder and generates a sequence of output tokens.\n",
      "\n",
      "The key mechanisms that make the Transformer work are:\n",
      "\n",
      "1. **Self-Attention Mechanism**: This allows the model to attend to different parts of the input sequence simultaneously, capturing long-range dependencies and contextual relationships between tokens.\n",
      "2. **Multi-Head Attention**: This mechanism enables the model to jointly attend to information from different representation subspaces at different positions, helping it capture more complex relationships between tokens.\n",
      "3. **Causal Masking**: This ensures that the decoder only attends to information up to the current position in the sequence, preventing it from looking ahead and generating output tokens prematurely.\n",
      "\n",
      "During inference, the model takes an input sequence and outputs a translated sequence of tokens by using the encoder-decoder structure and self-attention mechanism to generate each token. Two inference strategies are discussed: greedy search and beam search.\n",
      "\n",
      "Now, regarding the difference between the Transformer and RNN (Recurrent Neural Network):\n",
      "\n",
      "The key difference is that the Transformer does not rely on recurrent connections or a fixed-length input sequence, unlike traditional RNNs. Instead, it uses self-attention mechanisms to process input sequences of arbitrary length. This makes the Transformer more parallelizable and efficient for long-range dependencies.\n",
      "\n",
      "In contrast, RNNs are designed to handle sequential data by using recurrent connections to capture temporal relationships between tokens. While RNNs can be effective for certain tasks, they may struggle with longer-range dependencies or input sequences of varying lengths.\n"
     ]
    }
   ],
   "source": [
    "results = chroma_collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=5,\n",
    "    include=['documents', 'distances', 'metadatas']\n",
    ")\n",
    "\n",
    "context_doc = results['documents'][0][0]\n",
    "answer_prompt = f\"Answer the QUESTION using DOCUMENT as context.\\nQUESTION: {query_text}\\nDOCUMENT: {context_doc}\"\n",
    "\n",
    "response = ollama.chat(model=\"llama3\", messages=[\n",
    "    {\"role\": \"user\", \"content\": answer_prompt}\n",
    "])\n",
    "\n",
    "print(response[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
